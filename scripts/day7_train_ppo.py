#!/usr/bin/env python3
"""
Day 7 - ä»»åŠ¡2: PPOè®­ç»ƒè„šæœ¬
è®­ç»ƒPPOç­–ç•¥ç”¨äºå…±äº«å•è½¦è°ƒåº¦
"""

import sys
import os
from pathlib import Path
from datetime import datetime
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import (
    EvalCallback, 
    CheckpointCallback,
    CallbackList
)
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv

from simulator.bike_env import BikeRebalancingEnv


class PPOTrainer:
    """PPOè®­ç»ƒå™¨"""
    
    def __init__(self, config_path='config/env_config.yaml'):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        
        self.project_root = project_root
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = project_root / 'results' / 'ppo_training'
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.models_dir = self.output_dir / 'models'
        self.models_dir.mkdir(exist_ok=True)
        
        self.logs_dir = self.output_dir / 'logs'
        self.logs_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®
        print("="*70)
        print("Day 7 - PPOè®­ç»ƒç³»ç»Ÿ")
        print("="*70)
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {self.models_dir}")
        print(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.logs_dir}")
        
        print("\nğŸ“„ åŠ è½½é…ç½®...")
        config_full_path = project_root / config_path
        with open(config_full_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {config_full_path}")
        
    def create_env(self, scenario='default', monitor=True):
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        print(f"\nğŸ—ï¸  åˆ›å»ºç¯å¢ƒ (scenario={scenario})...")
        
        # åˆ›å»ºç¯å¢ƒ
        env = BikeRebalancingEnv(config_dict=self.config, scenario=scenario)
        
        # æ·»åŠ MonitoråŒ…è£…ï¼ˆç”¨äºè®°å½•å¥–åŠ±ï¼‰
        if monitor:
            log_path = self.logs_dir / f'env_{scenario}'
            log_path.mkdir(exist_ok=True)
            env = Monitor(env, str(log_path))
            print(f"   âœ… Monitorå·²å¯ç”¨: {log_path}")
        
        print("âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        return env
    
    def create_model(self, env, hyperparams=None):
        """åˆ›å»ºPPOæ¨¡å‹"""
        print("\nğŸ¤– åˆ›å»ºPPOæ¨¡å‹...")
        
        # é»˜è®¤è¶…å‚æ•°
        default_hyperparams = {
            'learning_rate': 3e-4,
            'n_steps': 2048,
            'batch_size': 64,
            'n_epochs': 10,
            'gamma': 0.99,
            'gae_lambda': 0.95,
            'clip_range': 0.2,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'verbose': 1,
            'tensorboard_log': str(self.logs_dir)
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„è¶…å‚æ•°
        if hyperparams:
            default_hyperparams.update(hyperparams)
        
        print("\nè¶…å‚æ•°é…ç½®:")
        for key, value in default_hyperparams.items():
            print(f"   {key}: {value}")
        
        # åˆ›å»ºæ¨¡å‹
        model = PPO(
            policy='MultiInputPolicy',
            env=env,
            **default_hyperparams
        )
        
        print("âœ… PPOæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        return model
    
    def create_callbacks(self, eval_env):
        """åˆ›å»ºè®­ç»ƒå›è°ƒ"""
        print("\nğŸ“‹ é…ç½®è®­ç»ƒå›è°ƒ...")
        
        callbacks = []
        
        # 1. è¯„ä¼°å›è°ƒ
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=str(self.models_dir / 'best_model'),
            log_path=str(self.logs_dir / 'eval'),
            eval_freq=10000,  # æ¯10000æ­¥è¯„ä¼°ä¸€æ¬¡
            n_eval_episodes=5,
            deterministic=True,
            render=False,
            verbose=1
        )
        callbacks.append(eval_callback)
        print("   âœ… è¯„ä¼°å›è°ƒå·²é…ç½®")
        
        # 2. æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=20000,  # æ¯20000æ­¥ä¿å­˜ä¸€æ¬¡
            save_path=str(self.models_dir / 'checkpoints'),
            name_prefix='ppo_bike'
        )
        callbacks.append(checkpoint_callback)
        print("   âœ… æ£€æŸ¥ç‚¹å›è°ƒå·²é…ç½®")
        
        return CallbackList(callbacks)
    
    def train(self, total_timesteps=100000, hyperparams=None):
        """è®­ç»ƒPPOæ¨¡å‹"""
        
        print("\n" + "="*70)
        print("å¼€å§‹è®­ç»ƒ")
        print("="*70)
        
        # åˆ›å»ºè®­ç»ƒå’Œè¯„ä¼°ç¯å¢ƒ
        print("\nğŸ“¦ å‡†å¤‡ç¯å¢ƒ...")
        train_env = self.create_env(scenario='default', monitor=True)
        eval_env = self.create_env(scenario='default', monitor=False)
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model(train_env, hyperparams)
        
        # åˆ›å»ºå›è°ƒ
        callbacks = self.create_callbacks(eval_env)
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (total_timesteps={total_timesteps})...")
        print("="*70)
        
        try:
            model.learn(
                total_timesteps=total_timesteps,
                callback=callbacks,
                log_interval=10,
                progress_bar=True
            )
            
            print("\n" + "="*70)
            print("âœ… è®­ç»ƒå®Œæˆï¼")
            print("="*70)
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = self.models_dir / f'ppo_final_{self.timestamp}'
            model.save(final_model_path)
            print(f"\nğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}.zip")
            
            return model
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
            
            # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
            interrupt_model_path = self.models_dir / f'ppo_interrupted_{self.timestamp}'
            model.save(interrupt_model_path)
            print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupt_model_path}.zip")
            
            return model
        
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def quick_test(self, model):
        """å¿«é€Ÿæµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("\n" + "="*70)
        print("å¿«é€Ÿæµ‹è¯•")
        print("="*70)
        
        env = self.create_env(scenario='default', monitor=False)
        
        print("\nè¿è¡Œ1ä¸ªepisode...")
        obs, info = env.reset(seed=42)
        
        total_reward = 0
        total_served = 0
        total_cost = 0
        step = 0
        done = False
        
        while not done:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            total_reward += reward
            total_served += info.get('served', 0)
            total_cost += info.get('rebalance_cost', 0)
            step += 1
        
        service_rate = info.get('service_rate', 0)
        
        print(f"\næµ‹è¯•ç»“æœ:")
        print(f"   æ€»æ­¥æ•°: {step}")
        print(f"   æ€»å¥–åŠ±: {total_reward:.2f}")
        print(f"   æœåŠ¡ç‡: {service_rate*100:.2f}%")
        print(f"   æ»¡è¶³éœ€æ±‚: {total_served:.0f}")
        print(f"   è°ƒåº¦æˆæœ¬: ${total_cost:.2f}")
        
        print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ")


def main():
    parser = argparse.ArgumentParser(description='PPOè®­ç»ƒè„šæœ¬')
    parser.add_argument('--timesteps', type=int, default=100000,
                       help='æ€»è®­ç»ƒæ­¥æ•° (é»˜è®¤: 100000)')
    parser.add_argument('--lr', type=float, default=3e-4,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 3e-4)')
    parser.add_argument('--n-steps', type=int, default=2048,
                       help='æ¯æ¬¡æ›´æ–°çš„æ­¥æ•° (é»˜è®¤: 2048)')
    parser.add_argument('--batch-size', type=int, default=64,
                       help='æ‰¹å¤§å° (é»˜è®¤: 64)')
    parser.add_argument('--quick-test', action='store_true',
                       help='è®­ç»ƒåè¿›è¡Œå¿«é€Ÿæµ‹è¯•')
    parser.add_argument('--test-only', type=str, default=None,
                       help='åªæµ‹è¯•å·²æœ‰æ¨¡å‹ (æä¾›æ¨¡å‹è·¯å¾„)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PPOTrainer()
    
    # åªæµ‹è¯•æ¨¡å¼
    if args.test_only:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼: åŠ è½½æ¨¡å‹ {args.test_only}")
        model = PPO.load(args.test_only)
        trainer.quick_test(model)
        return
    
    # è®­ç»ƒæ¨¡å¼
    hyperparams = {
        'learning_rate': args.lr,
        'n_steps': args.n_steps,
        'batch_size': args.batch_size
    }
    
    model = trainer.train(
        total_timesteps=args.timesteps,
        hyperparams=hyperparams
    )
    
    # å¿«é€Ÿæµ‹è¯•
    if model and args.quick_test:
        trainer.quick_test(model)
    
    print("\n" + "="*70)
    print("Day 7 è®­ç»ƒä»»åŠ¡å®Œæˆï¼")
    print("="*70)
    print("\nğŸ“Š æŸ¥çœ‹è®­ç»ƒæ—¥å¿—:")
    print(f"   tensorboard --logdir {trainer.logs_dir}")
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   æ¨¡å‹: {trainer.models_dir}")
    print(f"   æ—¥å¿—: {trainer.logs_dir}")


if __name__ == '__main__':
    main()