#!/usr/bin/env python3
"""
Day 8 - æˆæœ¬æ„ŸçŸ¥PPOè®­ç»ƒè„šæœ¬
ä½¿ç”¨æ”¹è¿›çš„å¥–åŠ±å‡½æ•°ï¼Œå¢åŠ è°ƒåº¦æˆæœ¬çš„æƒ©ç½šæƒé‡
"""

import os
import sys
import argparse
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from simulator.bike_env import BikeRebalancingEnv
from simulator.utils import load_config


def parse_args():
    parser = argparse.ArgumentParser(description='æˆæœ¬æ„ŸçŸ¥PPOè®­ç»ƒ')
    parser.add_argument('--timesteps', type=int, default=100000,
                       help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--lr', type=float, default=3e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--n-steps', type=int, default=2048,
                       help='æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°')
    parser.add_argument('--batch-size', type=int, default=64,
                       help='æ‰¹å¤§å°')
    parser.add_argument('--cost-weight', type=float, default=2.0,
                       help='è°ƒåº¦æˆæœ¬æƒé‡ï¼ˆåŸå§‹1.0ï¼Œå»ºè®®2.0-3.0ï¼‰')
    parser.add_argument('--penalty-weight', type=float, default=5.0,
                       help='æœªæ»¡è¶³éœ€æ±‚æƒ©ç½šæƒé‡')
    parser.add_argument('--revenue-weight', type=float, default=1.0,
                       help='æ”¶ç›Šæƒé‡')
    parser.add_argument('--ent-coef', type=float, default=0.0,
                       help='ç†µç³»æ•°ï¼ˆæ¢ç´¢æ€§ï¼‰')
    parser.add_argument('--quick-test', action='store_true',
                       help='è®­ç»ƒåè¿›è¡Œå¿«é€Ÿæµ‹è¯•')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    return parser.parse_args()


class CostAwareEnv(BikeRebalancingEnv):
    """æˆæœ¬æ„ŸçŸ¥çš„ç¯å¢ƒåŒ…è£…å™¨"""
    
    def __init__(self, config, scenario='default', 
                 cost_weight=2.0, penalty_weight=5.0, revenue_weight=1.0):
        super().__init__(config_dict=config, scenario=scenario)
        self.cost_weight = cost_weight
        self.penalty_weight = penalty_weight
        self.revenue_weight = revenue_weight
        
        print(f"[CostAwareEnv] å¥–åŠ±æƒé‡é…ç½®:")
        print(f"  - æ”¶ç›Šæƒé‡: {revenue_weight}")
        print(f"  - æˆæœ¬æƒé‡: {cost_weight}")
        print(f"  - æƒ©ç½šæƒé‡: {penalty_weight}")
        print(f"  - æ–°å¥–åŠ±å‡½æ•°: {revenue_weight}*revenue - {penalty_weight}*penalty - {cost_weight}*cost")
    
    def step(self, action):
        """é‡å†™stepæ–¹æ³•ï¼Œä½¿ç”¨æ–°çš„å¥–åŠ±å‡½æ•°"""
        obs, _, done, truncated, info = super().step(action)
        
        # æå–åŸå§‹å¥–åŠ±ç»„ä»¶
        revenue = info.get('revenue', 0)
        penalty = info.get('penalty', 0)
        rebalance_cost = info.get('rebalance_cost', 0)
        
        # ä½¿ç”¨æ–°çš„å¥–åŠ±å‡½æ•°
        new_reward = (
            self.revenue_weight * revenue -
            self.penalty_weight * penalty -
            self.cost_weight * rebalance_cost
        )
        
        # æ›´æ–°info
        info['reward_components'] = {
            'revenue': revenue,
            'penalty': penalty,
            'cost': rebalance_cost,
            'weighted_reward': new_reward
        }
        
        return obs, new_reward, done, truncated, info


def create_cost_aware_env(config, cost_weight, penalty_weight, revenue_weight):
    """åˆ›å»ºæˆæœ¬æ„ŸçŸ¥ç¯å¢ƒ"""
    return CostAwareEnv(
        config=config,
        scenario='default',
        cost_weight=cost_weight,
        penalty_weight=penalty_weight,
        revenue_weight=revenue_weight
    )


def quick_test(model, config, episodes=3):
    """å¿«é€Ÿæµ‹è¯•è®­ç»ƒçš„æ¨¡å‹"""
    print("\n" + "="*70)
    print("å¿«é€Ÿæµ‹è¯•")
    print("="*70 + "\n")
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒï¼ˆä½¿ç”¨æ ‡å‡†ç¯å¢ƒï¼‰
    test_env = BikeRebalancingEnv(config_dict=config, scenario='default')
    
    results = []
    for ep in range(episodes):
        obs, _ = test_env.reset()
        done = False
        episode_reward = 0
        episode_revenue = 0
        episode_cost = 0
        episode_served = 0
        episode_demand = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = test_env.step(action)
            
            episode_reward += reward
            episode_revenue += info.get('revenue', 0)
            episode_cost += info.get('rebalance_cost', 0)
            episode_served += info.get('total_served', 0)
            episode_demand += info.get('total_demand', 0)
            
            if done or truncated:
                break
        
        service_rate = episode_served / max(episode_demand, 1)
        net_profit = episode_revenue - episode_cost
        
        results.append({
            'episode': ep + 1,
            'reward': episode_reward,
            'service_rate': service_rate,
            'net_profit': net_profit,
            'cost': episode_cost
        })
        
        print(f"  Episode {ep+1}/{episodes}: "
              f"æœåŠ¡ç‡={service_rate*100:.1f}%, "
              f"å‡€åˆ©æ¶¦=${net_profit:.0f}, "
              f"æˆæœ¬=${episode_cost:.0f}")
    
    # è®¡ç®—å¹³å‡å€¼
    import numpy as np
    avg_service_rate = np.mean([r['service_rate'] for r in results])
    avg_net_profit = np.mean([r['net_profit'] for r in results])
    avg_cost = np.mean([r['cost'] for r in results])
    
    print(f"\nå¹³å‡è¡¨ç°:")
    print(f"  æœåŠ¡ç‡: {avg_service_rate*100:.2f}%")
    print(f"  å‡€åˆ©æ¶¦: ${avg_net_profit:.2f}")
    print(f"  è°ƒåº¦æˆæœ¬: ${avg_cost:.2f}")
    print()
    
    return results


def main():
    args = parse_args()
    
    print("="*70)
    print("Day 8 - æˆæœ¬æ„ŸçŸ¥PPOè®­ç»ƒ")
    print("="*70)
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("results/ppo_cost_aware")
    models_dir = output_dir / "models"
    logs_dir = output_dir / "logs"
    
    for d in [output_dir, models_dir, logs_dir]:
        d.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print()
    
    # åŠ è½½é…ç½®
    print("ğŸ“„ åŠ è½½é…ç½®...")
    config = load_config()
    print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print()
    
    # æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print("="*70)
    print("è®­ç»ƒé…ç½®")
    print("="*70)
    print()
    print(f"æ€»æ­¥æ•°: {args.timesteps:,}")
    print(f"å­¦ä¹ ç‡: {args.lr}")
    print(f"n_steps: {args.n_steps}")
    print(f"batch_size: {args.batch_size}")
    print(f"ç†µç³»æ•°: {args.ent_coef}")
    print(f"éšæœºç§å­: {args.seed}")
    print()
    print("å¥–åŠ±å‡½æ•°æƒé‡:")
    print(f"  - æ”¶ç›Š: {args.revenue_weight}")
    print(f"  - æˆæœ¬: {args.cost_weight} (åŸå§‹1.0ï¼Œæé«˜{args.cost_weight}x)")
    print(f"  - æƒ©ç½š: {args.penalty_weight}")
    print()
    print(f"æ–°å¥–åŠ±å‡½æ•°: reward = {args.revenue_weight}*revenue - {args.penalty_weight}*penalty - {args.cost_weight}*cost")
    print()
    
    # åˆ›å»ºç¯å¢ƒ
    print("="*70)
    print("åˆ›å»ºè®­ç»ƒç¯å¢ƒ")
    print("="*70)
    print()
    
    def make_env():
        return create_cost_aware_env(
            config,
            cost_weight=args.cost_weight,
            penalty_weight=args.penalty_weight,
            revenue_weight=args.revenue_weight
        )
    
    env = DummyVecEnv([make_env])
    print("âœ… è®­ç»ƒç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = DummyVecEnv([make_env])
    
    # åˆ›å»ºPPOæ¨¡å‹
    print("="*70)
    print("åˆå§‹åŒ–PPOæ¨¡å‹")
    print("="*70)
    print()
    
    model = PPO(
        "MultiInputPolicy",
        env,
        learning_rate=args.lr,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        ent_coef=args.ent_coef,
        verbose=1,
        tensorboard_log=str(logs_dir),
        seed=args.seed
    )
    
    print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    print()
    
    # åˆ›å»ºå›è°ƒ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(models_dir / "best_model"),
        log_path=str(logs_dir / "eval"),
        eval_freq=max(args.timesteps // 10, 1000),
        n_eval_episodes=5,
        deterministic=True,
        render=False,
        verbose=1
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=max(args.timesteps // 5, 5000),
        save_path=str(models_dir / "checkpoints"),
        name_prefix=f"ppo_cost_aware",
        verbose=1
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("="*70)
    print("å¼€å§‹è®­ç»ƒ")
    print("="*70)
    print()
    print(f"ğŸš€ å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š å¯ä»¥ä½¿ç”¨TensorBoardæŸ¥çœ‹è®­ç»ƒè¿›åº¦:")
    print(f"   tensorboard --logdir {logs_dir.absolute()}")
    print()
    
    model.learn(
        total_timesteps=args.timesteps,
        callback=[eval_callback, checkpoint_callback],
        progress_bar=True
    )
    
    print()
    print(f"âœ… è®­ç»ƒå®Œæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = models_dir / f"ppo_cost_aware_final_{timestamp}.zip"
    model.save(str(final_model_path))
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    print()
    
    # å¿«é€Ÿæµ‹è¯•
    if args.quick_test:
        quick_test(model, config, episodes=3)
    
    print("="*70)
    print("âœ… Day 8 æˆæœ¬æ„ŸçŸ¥è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print()
    print("ğŸ“‚ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - æœ€ä½³æ¨¡å‹: {models_dir / 'best_model' / 'best_model.zip'}")
    print(f"  - æœ€ç»ˆæ¨¡å‹: {final_model_path}")
    print(f"  - è®­ç»ƒæ—¥å¿—: {logs_dir}")
    print()
    print("ğŸ¯ ä¸‹ä¸€æ­¥:")
    print("  1. è¿è¡Œè¯„ä¼°è„šæœ¬å¯¹æ¯”æ€§èƒ½")
    print("  2. å¦‚æœæˆæœ¬ä»é«˜ï¼Œå°è¯•å¢åŠ  --cost-weight")
    print("  3. å¦‚æœæœåŠ¡ç‡ä¸‹é™ï¼Œå°è¯•å¢åŠ  --penalty-weight")
    print()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())